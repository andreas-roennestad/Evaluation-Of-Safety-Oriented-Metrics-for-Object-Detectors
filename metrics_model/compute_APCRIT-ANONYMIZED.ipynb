{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b837670e",
   "metadata": {},
   "source": [
    "### Evaluation of criticality metrics on the nuscenes dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2bcfd7",
   "metadata": {},
   "source": [
    "**Results produced for the nuscenes dataset are needed, as for example they can be produced by the notebook MMDetection3D, see the other notebook MMDetection3D**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d6f77e",
   "metadata": {},
   "source": [
    "**Note: nuscenes-dev must be correctly installed** We used version 1.1.2, but it has been tested up to version 1.1.7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3bfeea",
   "metadata": {},
   "source": [
    "#### Some hints on the modifications done to the nuscenes library:\n",
    "\n",
    "- detection/algo.py : the method \"accumulate\" computes precision, recall and average precision. The approach to compute precision and recall is as follows. NPOS=TP+FN is a static value and can be computed immediately. Then, an array with all the detection thresholds for tps is computed e.g. $[1\\; 0\\; 0\\; 0\\; 1]$. Same is done for fp. Same is done with the detection confidence. Then they are sorted according to the detection confidence, and elements are picked up from top to bottom. Precision and recall are computed with the retrieved items, iteratively. This is the equivalent of starting getting \"detected items\" with the highest confidence, and progressively get all the remaining items, and compute precision and recall at each step with all the items selected so far. To compute the precision curve, there is a marvellous interpolation of the precision matrix and the recall matrix with a predefined matrix.\n",
    "\n",
    "\n",
    "- detection/evaluate.py : contains the main method\n",
    "\n",
    "\n",
    "- detection/data_classes.py : DetectionBox, in the init method, it includes all the logic to perform computation of all criticalities for a target BoundingBox i.e., for each object. This is done independently if it is a ground truth bounding box or a predicted bounding box. So whenever we plan to change our geometry, we need to act here.\n",
    "\n",
    "\n",
    "Further, image creation is manipulated so that it shows criticality values matched to each bounding box that is depicted.  It visualizes only one type of objects (CAR), for clarity of the image --> to change this, in utils/data_classes.py, render_crit, change the line \"if(self.name!='car'):\"\n",
    "\n",
    "Programmatically, the base is visualize_sample_crit in detection/render.py; then there is method render_crit; and finally we build object Box in utils/data_classes.py; images are created at the beginning of main in detection/evaluate.py; the label is added in render_crit in utils/data_classes.py. To print a different class (e.g., car, bus, everything), should be easy working with the above files (the no_crit case prints everything is on map).\n",
    "\n",
    "**Debug images** Debug images (folder examples_debug_1) are introduced. They consider only ground truth, and they print: velocity and position of all vehicles,including the ego vehicle. They are created as follows:\n",
    "- in detection/evaluate.py there is method visualize_sample_debug_1 of eval.detection.render\n",
    "- in eval.detection.render method visualize_sample_debug_1, for each boxes in gt, there is render_debug_1 of utils/data_classes.py\n",
    "- utils/data_classes.py method render_debug_1 builds the squares of the different ground truth cars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3cf5bf",
   "metadata": {},
   "source": [
    "#### Which results are computed\n",
    "\n",
    "Results are saved in different folders. They are easy to find, you just need to configure the proper path as it will be explained a bit later here.\n",
    "\n",
    "\n",
    "We collect results in terms of:\n",
    "\n",
    "- precision recall curve, that we save as pdf files (see the pdf files with \"crit\" in the name)\n",
    "\n",
    "We compute some more metrics in a text file, that is called \"confusion matrix.txt\". The file contains: i) average precision; ii) average precision crit;\n",
    "\n",
    "*Example: class_name bus; dist_th 1.0; ap 0.16393238098925264; ap_crit 0.16604208258718856*\n",
    "\n",
    "iii) MAXIMUM number of tp and fp; iv) MAXIMUM value of tp_crit pred and MAXIMUM value of tp_crit gt; iii) MAXIMUM number of tp; iv) MAXIMUM value of fp_crit pred; v) MINIMUM number of FN, and MINIMUM value of fn_crit gt\n",
    "\n",
    "*Example: class_name car; dist_th 0.5; max tp 19758.0; max fp 177143.0; min fn 33285.0; max tp_pred_crit 19249.891210080583; max tp_gt_crit 19272.72239514313; max fp_pred_crit 172455.20687223336; min fn_gt_crit 32468.569145629514*\n",
    "\n",
    "To enrich this with multiple metrics, just operate on method *accumulate* in *algo.py*\n",
    "\n",
    "Last, you can save bird eye view images that have the crit values added to each represented bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f85fcf",
   "metadata": {},
   "source": [
    "ACRONYMS (as naming in the following differs with respect to the paper)\n",
    "\n",
    "First column is the algorithm name (as close as possible to naming in mmdetection3d), second is the abbreviation we use in our paper\n",
    "\n",
    "- FCOSD-RESNET101              FCOS\n",
    "- pointpillars-secfpn          SEC\n",
    "- pointpillars-fpn             FPN\n",
    "- regnet-regnetX_400MF-FPN     REG400\n",
    "- ssn-SECFPN                   SSN\n",
    "- regnet-regnetX_400MF-SECFPN  REGSEC (REG400SEC)\n",
    "- ssn-REGNET                   SSNREG\n",
    "- regnet-regnetX_FPN           REG1.6\n",
    "- pgd                          PGD\n",
    "\n",
    "\n",
    "Also, we have:\n",
    "\n",
    "- D in the paper is:    $D_{max}$\n",
    "- T in the paper is:        $T_{max}$\n",
    "- I in the paper is:        $R_{max}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a082072",
   "metadata": {},
   "source": [
    "*reminder:* if you ever need to change dist_ths, dist_thp: check configs/detection_cvpr_2019.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc3c37b",
   "metadata": {},
   "source": [
    "#### Below are configuration items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece0241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## POINTPILLARS WITH FPN BACKBONE -- lidar only\n",
    "#PATH='/cluster/work/andronn/MasterThesis/MASTER/mmdetection3d/results/pointpillars_nuscenes_results/pts_bbox/'\n",
    "#FILE_JSON='results_nusc.json'\n",
    "#ETECTOR='pointpillars-fpn'\n",
    "\n",
    "##RegNET WITH BACKBONE RegNetX-1.6gF-FPN\n",
    "PATH='/cluster/work/andronn/MasterThesis/MASTER/mmdetection3d/results/regnetX_nuscenes_results/pts_bbox/' \n",
    "FILE_JSON='results_nusc.json'\n",
    "DETECTOR='regnet-regnetX_FPN'\n",
    "\n",
    "#### SSN con backbone REGNET 400MF SECFPN  -- LIDAR-only\n",
    "#PATH='/cluster/work/andronn/MasterThesis/MASTER/mmdetection3d/results/ssn_nuscenes_results/pts_bbox/' \n",
    "#FILE_JSON='results_nusc.json'\n",
    "#DETECTOR='ssn-REGNET'\n",
    "\n",
    "\n",
    "#nuscene dataroot\n",
    "DATAROOT = '/cluster/work/andronn/MasterThesis/MASTER/mmdetection3d/data/nuscenes'\n",
    "#path + json file where detection results from mmdetection3d are stored, ready to be processed\n",
    "\n",
    "RESULT_PATH=PATH+FILE_JSON\n",
    "#the detector in use. Does nothing special but creates a folder, and it is useful to put results there\n",
    "#results of evaluation will be stored here\n",
    "OUTPUT='/cluster/work/andronn/MasterThesis/MASTER/master_repo/Thesis-Evaluating-Safety-Oriented-Metrics-for-Object-Detectors/results/'+DETECTOR+\"/\"\n",
    "\n",
    "#parameters from our marvellous solution. At the bottom of the notebook, there is a different \n",
    "#approach where multiple config are tested\n",
    "MAX_DISTANCE_OBJ_1=30.0\n",
    "MAX_DISTANCE_INTERSECT_1=20.0\n",
    "MAX_TIME_INTERSECT_OBJ_1=10.0\n",
    "\n",
    "#how many images in bird view you want to draw\n",
    "NUMBER_IMAGE=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3f54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#required if you are playing with libraries and changing them. This reloads libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f36ee733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import tqdm\n",
    "import pandas\n",
    "import math\n",
    "import json\n",
    "from typing import Callable\n",
    "from nuscenes import NuScenes\n",
    "from nuscenes.eval.prediction.splits import *\n",
    "import nuscenes.eval.detection.config as cnfig\n",
    "from nuscenes.eval.detection.configs import *\n",
    "from nuscenes.eval.detection.data_classes import DetectionBox \n",
    "from nuscenes.eval.detection import *\n",
    "import nuscenes.eval.detection.algo as ag\n",
    "from nuscenes.eval.detection.data_classes import DetectionMetricData, DetectionConfig, DetectionMetrics, DetectionBox, \\\n",
    "    DetectionMetricDataList\n",
    "from nuscenes.eval.common.data_classes import EvalBoxes\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Callable, Tuple\n",
    "from nuscenes.eval.common.utils import center_distance, scale_iou, yaw_diff, velocity_l2, attr_acc, cummean\n",
    "import nuscenes.eval.detection.evaluate as dcl    \n",
    "from nuscenes.prediction import *\n",
    "from nuscenes.map_expansion.map_api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a3dcbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 50.834 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 13.9 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "nuscenes = NuScenes('v1.0-trainval', dataroot=DATAROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d043295",
   "metadata": {},
   "outputs": [],
   "source": [
    "confvalue=cnfig.config_factory(\"detection_cvpr_2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1685adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of scene that compose the val set\n",
    "eval=val = \\\n",
    "    ['scene-0003', 'scene-0012', 'scene-0013', 'scene-0014', 'scene-0015', 'scene-0016', 'scene-0017', 'scene-0018',\n",
    "     'scene-0035', 'scene-0036', 'scene-0038', 'scene-0039', 'scene-0092', 'scene-0093', 'scene-0094', 'scene-0095',\n",
    "     'scene-0096', 'scene-0097', 'scene-0098', 'scene-0099', 'scene-0100', 'scene-0101', 'scene-0102', 'scene-0103',\n",
    "     'scene-0104', 'scene-0105', 'scene-0106', 'scene-0107', 'scene-0108', 'scene-0109', 'scene-0110', 'scene-0221',\n",
    "     'scene-0268', 'scene-0269', 'scene-0270', 'scene-0271', 'scene-0272', 'scene-0273', 'scene-0274', 'scene-0275',\n",
    "     'scene-0276', 'scene-0277', 'scene-0278', 'scene-0329', 'scene-0330', 'scene-0331', 'scene-0332', 'scene-0344',\n",
    "     'scene-0345', 'scene-0346', 'scene-0519', 'scene-0520', 'scene-0521', 'scene-0522', 'scene-0523', 'scene-0524',\n",
    "     'scene-0552', 'scene-0553', 'scene-0554', 'scene-0555', 'scene-0556', 'scene-0557', 'scene-0558', 'scene-0559',\n",
    "     'scene-0560', 'scene-0561', 'scene-0562', 'scene-0563', 'scene-0564', 'scene-0565', 'scene-0625', 'scene-0626',\n",
    "     'scene-0627', 'scene-0629', 'scene-0630', 'scene-0632', 'scene-0633', 'scene-0634', 'scene-0635', 'scene-0636',\n",
    "     'scene-0637', 'scene-0638', 'scene-0770', 'scene-0771', 'scene-0775', 'scene-0777', 'scene-0778', 'scene-0780',\n",
    "     'scene-0781', 'scene-0782', 'scene-0783', 'scene-0784', 'scene-0794', 'scene-0795', 'scene-0796', 'scene-0797',\n",
    "     'scene-0798', 'scene-0799', 'scene-0800', 'scene-0802', 'scene-0904', 'scene-0905', 'scene-0906', 'scene-0907',\n",
    "     'scene-0908', 'scene-0909', 'scene-0910', 'scene-0911', 'scene-0912', 'scene-0913', 'scene-0914', 'scene-0915',\n",
    "     'scene-0916', 'scene-0917', 'scene-0919', 'scene-0920', 'scene-0921', 'scene-0922', 'scene-0923', 'scene-0924',\n",
    "     'scene-0925', 'scene-0926', 'scene-0927', 'scene-0928', 'scene-0929', 'scene-0930', 'scene-0931', 'scene-0962',\n",
    "     'scene-0963', 'scene-0966', 'scene-0967', 'scene-0968', 'scene-0969', 'scene-0971', 'scene-0972', 'scene-1059',\n",
    "     'scene-1060', 'scene-1061', 'scene-1062', 'scene-1063', 'scene-1064', 'scene-1065', 'scene-1066', 'scene-1067',\n",
    "     'scene-1068', 'scene-1069', 'scene-1070', 'scene-1071', 'scene-1072', 'scene-1073']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea963f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "476fc090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                 \r"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "dt=dcl.DetectionEval(nuscenes,confvalue, RESULT_PATH, 'val', OUTPUT, verbose=False, MAX_DISTANCE_OBJ=MAX_DISTANCE_OBJ_1,\n",
    "                     MAX_DISTANCE_INTERSECT=MAX_DISTANCE_INTERSECT_1, MAX_TIME_INTERSECT_OBJ=MAX_TIME_INTERSECT_OBJ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27aac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of nuscenes.eval.detection.algo failed: Traceback (most recent call last):\n",
      "  File \"/cluster/home/andronn/.local/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 261, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/cluster/home/andronn/.local/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 459, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/cluster/apps/eb/software/Anaconda3/2020.07/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 779, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 916, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 846, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/cluster/home/andronn/.local/lib/python3.8/site-packages/nuscenes/eval/detection/algo.py\", line 248\n",
      "    return DetectionMetricData(recall=rec,\n",
      "    ^\n",
      "IndentationError: expected an indented block\n",
      "]\n",
      "[autoreload of nuscenes.eval.detection.evaluate failed: Traceback (most recent call last):\n",
      "  File \"/cluster/home/andronn/.local/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 261, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/cluster/home/andronn/.local/lib/python3.8/site-packages/IPython/extensions/autoreload.py\", line 459, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/cluster/apps/eb/software/Anaconda3/2020.07/lib/python3.8/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 604, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 779, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 916, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 846, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/cluster/home/andronn/.local/lib/python3.8/site-packages/nuscenes/eval/detection/evaluate.py\", line 267\n",
      "    'pred_boxes_cars': [box.serialize() if box.detection_name=='car' for box in boxes_pred]}\n",
      "                                                                     ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING EVALUATION in main (self)\n",
      "Made sample directory: /cluster/work/andronn/MasterThesis/MASTER/master_repo/Thesis-Evaluating-Safety-Oriented-Metrics-for-Object-Detectors/results/regnet-regnetX_FPN/METRIC_SAMPLES/cb934ea6cdef412e9d8d6f0f41286fe2\n",
      "\n",
      "Rendering sample token cb934ea6cdef412e9d8d6f0f41286fe2\n",
      "Saved metric data for sample cb934ea6cdef412e9d8d6f0f41286fe2\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "#dt.main(plot_examples=NUMBER_IMAGE,render_curves=True)\n",
    "\n",
    "    \n",
    "dt.main(plot_examples=NUMBER_IMAGE,\n",
    "        render_curves=True, \n",
    "        model_name=DETECTOR,\n",
    "        MAX_DISTANCE_OBJ=MAX_DISTANCE_OBJ_1,\n",
    "        MAX_DISTANCE_INTERSECT=MAX_DISTANCE_INTERSECT_1,\n",
    "        MAX_TIME_INTERSECT=MAX_TIME_INTERSECT_OBJ_1,\n",
    "        recall_type=\"PRED AL NUMERATORE\",\n",
    "        save_metrics_samples=5) #This must be \"PRED AL NUMERATORE\", to match results of the paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb35f276",
   "metadata": {},
   "source": [
    "### Calculate PKLs\n",
    "Using planning_centric_metrics, calculate PKL for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7188c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nuscenes.map_expansion.map_api import NuScenesMap\n",
    "from planning_centric_metrics import calculate_pkl\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(torch.device('cpu'))\n",
    "print('using device: {0}'.format(device))\n",
    "\n",
    "map_folder = '/cluster/work/andronn/MasterThesis/MASTER/mmdetection3d/data/nuscenes/maps/'\n",
    "nusc_maps = {map_name: NuScenesMap(dataroot=map_folder,\n",
    "                 map_name=map_name) for map_name in [\n",
    "                    \"singapore-hollandvillage\",\n",
    "                    \"singapore-queenstown\",\n",
    "                    \"boston-seaport\",\n",
    "                    \"singapore-onenorth\",\n",
    "                ]}\n",
    "nworkers = 1\n",
    "\n",
    "pkl = calculate_pkl(dt.gt_boxes, dt.pred_boxes,\n",
    "                         dt.sample_tokens, dt.nusc,\n",
    "                         nusc_maps, device,\n",
    "                         nworkers=nworkers, bsz=128,\n",
    "                         plot_kextremes=0,\n",
    "                         verbose=True)\n",
    "with open(os.path.join(OUTPUT,'pkl_results.json'), 'w') as fp:\n",
    "    json.dump(pkl, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d6993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"VARIOUS_DISTANCE_OBJ_ARRAY=[]\n",
    "VARIOUS_DISTANCE_OBJ_ARRAY.extend(range(5, 51, 5))\n",
    "VARIOUS_DISTANCE_INTERSECT_ARRAY=[]\n",
    "VARIOUS_DISTANCE_INTERSECT_ARRAY.extend(range(5, 51, 5))\n",
    "VARIOUS_TIME_INTERSECT_ARRAY=[]\n",
    "VARIOUS_TIME_INTERSECT_ARRAY.extend(range(2,31,2))\n",
    "\n",
    "import itertools\n",
    "a = [VARIOUS_DISTANCE_OBJ_ARRAY,VARIOUS_DISTANCE_INTERSECT_ARRAY,VARIOUS_TIME_INTERSECT_ARRAY]\n",
    "combined_list=list(itertools.product(*a))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b576e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many images in bird view you want to draw\n",
    "\"\"\"NUMBER_IMAGE=0\n",
    "rendering=False\n",
    "len(combined_list)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab5431",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def compute_values(PATH, FILE_JSON, DETECTOR, RESULT_PATH, OUTPUT):\n",
    "    for j in range(len(combined_list)):\n",
    "        OUTPUT1=OUTPUT+'/'+'DATA'+'/iteration_'+str(j)+'/'\n",
    "\n",
    "        dt=dcl.DetectionEval(nuscenes,\n",
    "                             confvalue, RESULT_PATH, 'val', OUTPUT1, verbose=False,\n",
    "                             MAX_DISTANCE_OBJ=combined_list[j][0],\n",
    "                             MAX_DISTANCE_INTERSECT=combined_list[j][1], \n",
    "                             MAX_TIME_INTERSECT_OBJ=combined_list[j][2])\n",
    "    #    dt.main(plot_examples=NUMBER_IMAGE,render_curves=True)\n",
    "        dt.main(plot_examples=NUMBER_IMAGE,\n",
    "                render_curves=rendering, \n",
    "                model_name=DETECTOR,\n",
    "                MAX_DISTANCE_OBJ=combined_list[j][0],\n",
    "                MAX_DISTANCE_INTERSECT=combined_list[j][1],\n",
    "                MAX_TIME_INTERSECT=combined_list[j][2],\n",
    "                recall_type=\"PRED AL NUMERATORE\") \n",
    "        f = open(OUTPUT1+\"README_CONFIG.txt\", \"w\")\n",
    "        f.write(\"Configurations for this iteration\\n\")\n",
    "        f.write(\"RECALL TYPE: CRIT (AS THE PAPER)\\n\")\n",
    "        f.write(\"MAX_DISTANCE_OBJ \"+str(combined_list[j][0])+\"\\n\")\n",
    "        f.write(\"MAX_DISTANCE_INTERSECT \"+str(combined_list[j][1])+\"\\n\")\n",
    "        f.write(\"MAX_TIME_INTERSECT \"+str(combined_list[j][2])+\"\\n\")\n",
    "        f.close()\n",
    "        del dt\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81bae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PATH='/home/notebook/mmdetection3d/pgd_results/img_bbox/'\n",
    "FILE_JSON='results_nusc.json'\n",
    "DETECTOR='pgd'\n",
    "\n",
    "RESULT_PATH=PATH+FILE_JSON\n",
    "OUTPUT='/home/notebook/results/'+DETECTOR+\"/\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115839e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"%%capture\n",
    "%reload_ext autoreload\n",
    "compute_values(PATH, FILE_JSON, DETECTOR, RESULT_PATH, OUTPUT);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69690d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc2480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
