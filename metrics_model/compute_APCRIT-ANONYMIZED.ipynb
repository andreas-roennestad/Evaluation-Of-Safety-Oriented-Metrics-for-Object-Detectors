{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b837670e",
   "metadata": {},
   "source": [
    "### Evaluation of criticality metrics on the nuscenes dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2bcfd7",
   "metadata": {},
   "source": [
    "**Results produced for the nuscenes dataset are needed, as for example they can be produced by the notebook MMDetection3D, see the other notebook MMDetection3D**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d6f77e",
   "metadata": {},
   "source": [
    "**Note: nuscenes-dev must be correctly installed** We used version 1.1.2, but it has been tested up to version 1.1.7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3bfeea",
   "metadata": {},
   "source": [
    "#### Some hints on the modifications done to the nuscenes library:\n",
    "\n",
    "- detection/algo.py : the method \"accumulate\" computes precision, recall and average precision. The approach to compute precision and recall is as follows. NPOS=TP+FN is a static value and can be computed immediately. Then, an array with all the detection thresholds for tps is computed e.g. $[1\\; 0\\; 0\\; 0\\; 1]$. Same is done for fp. Same is done with the detection confidence. Then they are sorted according to the detection confidence, and elements are picked up from top to bottom. Precision and recall are computed with the retrieved items, iteratively. This is the equivalent of starting getting \"detected items\" with the highest confidence, and progressively get all the remaining items, and compute precision and recall at each step with all the items selected so far. To compute the precision curve, there is a marvellous interpolation of the precision matrix and the recall matrix with a predefined matrix.\n",
    "\n",
    "\n",
    "- detection/evaluate.py : contains the main method\n",
    "\n",
    "\n",
    "- detection/data_classes.py : DetectionBox, in the init method, it includes all the logic to perform computation of all criticalities for a target BoundingBox i.e., for each object. This is done independently if it is a ground truth bounding box or a predicted bounding box. So whenever we plan to change our geometry, we need to act here.\n",
    "\n",
    "\n",
    "Further, image creation is manipulated so that it shows criticality values matched to each bounding box that is depicted.  It visualizes only one type of objects (CAR), for clarity of the image --> to change this, in utils/data_classes.py, render_crit, change the line \"if(self.name!='car'):\"\n",
    "\n",
    "Programmatically, the base is visualize_sample_crit in detection/render.py; then there is method render_crit; and finally we build object Box in utils/data_classes.py; images are created at the beginning of main in detection/evaluate.py; the label is added in render_crit in utils/data_classes.py. To print a different class (e.g., car, bus, everything), should be easy working with the above files (the no_crit case prints everything is on map).\n",
    "\n",
    "**Debug images** Debug images (folder examples_debug_1) are introduced. They consider only ground truth, and they print: velocity and position of all vehicles,including the ego vehicle. They are created as follows:\n",
    "- in detection/evaluate.py there is method visualize_sample_debug_1 of eval.detection.render\n",
    "- in eval.detection.render method visualize_sample_debug_1, for each boxes in gt, there is render_debug_1 of utils/data_classes.py\n",
    "- utils/data_classes.py method render_debug_1 builds the squares of the different ground truth cars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3cf5bf",
   "metadata": {},
   "source": [
    "#### Which results are computed\n",
    "\n",
    "Results are saved in different folders. They are easy to find, you just need to configure the proper path as it will be explained a bit later here.\n",
    "\n",
    "\n",
    "We collect results in terms of:\n",
    "\n",
    "- precision recall curve, that we save as pdf files (see the pdf files with \"crit\" in the name)\n",
    "\n",
    "We compute some more metrics in a text file, that is called \"confusion matrix.txt\". The file contains: i) average precision; ii) average precision crit;\n",
    "\n",
    "*Example: class_name bus; dist_th 1.0; ap 0.16393238098925264; ap_crit 0.16604208258718856*\n",
    "\n",
    "iii) MAXIMUM number of tp and fp; iv) MAXIMUM value of tp_crit pred and MAXIMUM value of tp_crit gt; iii) MAXIMUM number of tp; iv) MAXIMUM value of fp_crit pred; v) MINIMUM number of FN, and MINIMUM value of fn_crit gt\n",
    "\n",
    "*Example: class_name car; dist_th 0.5; max tp 19758.0; max fp 177143.0; min fn 33285.0; max tp_pred_crit 19249.891210080583; max tp_gt_crit 19272.72239514313; max fp_pred_crit 172455.20687223336; min fn_gt_crit 32468.569145629514*\n",
    "\n",
    "To enrich this with multiple metrics, just operate on method *accumulate* in *algo.py*\n",
    "\n",
    "Last, you can save bird eye view images that have the crit values added to each represented bounding box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f85fcf",
   "metadata": {},
   "source": [
    "ACRONYMS (as naming in the following differs with respect to the paper)\n",
    "\n",
    "First column is the algorithm name (as close as possible to naming in mmdetection3d), second is the abbreviation we use in our paper\n",
    "\n",
    "- FCOSD-RESNET101              FCOS\n",
    "- pointpillars-secfpn          SEC\n",
    "- pointpillars-fpn             FPN\n",
    "- regnet-regnetX_400MF-FPN     REG400\n",
    "- ssn-SECFPN                   SSN\n",
    "- regnet-regnetX_400MF-SECFPN  REGSEC (REG400SEC)\n",
    "- ssn-REGNET                   SSNREG\n",
    "- regnet-regnetX_FPN           REG1.6\n",
    "- pgd                          PGD\n",
    "\n",
    "\n",
    "Also, we have:\n",
    "\n",
    "- D in the paper is:    $D_{max}$\n",
    "- T in the paper is:        $T_{max}$\n",
    "- I in the paper is:        $R_{max}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a082072",
   "metadata": {},
   "source": [
    "*reminder:* if you ever need to change dist_ths, dist_thp: check configs/detection_cvpr_2019.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a154ab0e",
   "metadata": {},
   "source": [
    "Before this notebook is run, predictions must be made over the dataset for each of the detectors. Lets say the detector we want to run is the regnetX-1.6gf model. To run the detections on the GPU in mmdetection3d, run the following command from the mmdetection3d root directory: \n",
    "\n",
    "\n",
    "#### 1 pointpillars with backbone FPN (Feature Pyramid Networks) \n",
    "Model: https://download.openmmlab.com/mmdetection3d/v1.0.0_models/pointpillars/hv_pointpillars_fpn_sbn-all_4x8_2x_nus-3d/hv_pointpillars_fpn_sbn-all_4x8_2x_nus-3d_20210826_104936-fca299c1.pth\n",
    "\n",
    "Results in: './pointpillars_nuscenes_results/pts_bbox/results_nusc.json' \n",
    "\n",
    "% ./tools/dist_test.sh configs/pointpillars/hv_pointpillars_fpn_sbn-all_4x8_2x_nus-3d.py checkpoints/hv_pointpillars_fpn_sbn-all_4x8_2x_nus-3d_20200620_230405-2fa62f3d.pth 2 --format-only --eval-options 'jsonfile_prefix=./results/pointpillars_nuscenes_results'\n",
    "\n",
    "mAP: 39.7%\n",
    "mAP: XX% (0.10 confidence filter)\n",
    "\n",
    "#### 2 RegNET-X (RegNET WITH 400MF-FPN)\n",
    "Model: https://github.com/open-mmlab/mmdetection3d/blob/master/configs/regnet/hv_pointpillars_regnet-400mf_fpn_sbn-all_4x8_2x_nus-3d.py\n",
    "\n",
    "Results in: './regnetX_nuscenes_results/pts_bbox/results_nusc.json'\n",
    "\n",
    "% ./tools/dist_test.sh configs/regnet/hv_pointpillars_regnet-400mf_fpn_sbn-all_4x8_2x_nus-3d.py checkpoints/hv_pointpillars_regnet-400mf_fpn_sbn-all_4x8_2x_nus-3d_20200620_230239-c694dce7.pth 2 --format-only --eval-options 'jsonfile_prefix=./results/regnetX_nuscenes_results'\n",
    "\n",
    "mAP: 44.84%\n",
    "mAP: XX% (0.10 confidence filter)\n",
    "\n",
    "#### 3 SSN with backbone REGNET \n",
    "Model: https://github.com/open-mmlab/mmdetection3d/blob/master/configs/ssn/hv_ssn_regnet-400mf_secfpn_sbn-all_2x16_2x_nus-3d.py\n",
    "\n",
    "Results in: './ssn_nuscenes_results/pts_bbox/results_nusc-regnet.json'\n",
    "\n",
    "% ./tools/dist_test.sh configs/ssn/hv_ssn_regnet-400mf_secfpn_sbn-all_2x16_2x_nus-3d.py checkpoints/hv_ssn_regnet-400mf_secfpn_sbn-all_2x16_2x_nus-3d_20201024_232447-7af3d8c8.pth 2 --format-only --eval-options 'jsonfile_prefix=./results/ssn_nuscenes_results'\n",
    "\n",
    "mAP:46.65%\n",
    "mAP: XX% (0.10 confidence filter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc3c37b",
   "metadata": {},
   "source": [
    "#### Below are configuration items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece0241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## POINTPILLARS WITH FPN BACKBONE -- lidar only\n",
    "PATH='/cluster/work/andronn/MasterThesis/MASTER/mmdetection3d/results/pointpillars_nuscenes_results/pts_bbox/'\n",
    "FILE_JSON='results_nusc.json'\n",
    "DETECTOR='pointpillars-fpn'\n",
    "\n",
    "##RegNET WITH BACKBONE RegNetX-1.6gF-FPN\n",
    "#PATH='/cluster/work/andronn/MasterThesis/MASTER/mmdetection3d/results/regnetX_nuscenes_results/pts_bbox/' \n",
    "#FILE_JSON='results_nusc.json'\n",
    "#DETECTOR='regnet-regnetX_FPN'\n",
    "\n",
    "### SSN con backbone REGNET 400MF SECFPN  -- LIDAR-only\n",
    "#PATH='/cluster/work/andronn/MasterThesis/MASTER/mmdetection3d/results/ssn_nuscenes_results/pts_bbox/' \n",
    "#FILE_JSON='results_nusc.json'\n",
    "#DETECTOR='ssn-REGNET'\n",
    "\n",
    "#nuscene dataroot\n",
    "DATAROOT = '/cluster/work/andronn/MasterThesis/MASTER/mmdetection3d/data/nuscenes'\n",
    "#path + json file where detection results from mmdetection3d are stored, ready to be processed\n",
    "\n",
    "RESULT_PATH=PATH+FILE_JSON\n",
    "#the detector in use. Does nothing special but creates a folder, and it is useful to put results there\n",
    "#results of evaluation will be stored here\n",
    "OUTPUT='/cluster/work/andronn/MasterThesis/MASTER/master_repo/Thesis-Evaluating-Safety-Oriented-Metrics-for-Object-Detectors/results/'+DETECTOR+\"/\"\n",
    "\n",
    "#parameters from our marvellous solution. At the bottom of the notebook, there is a different \n",
    "#approach where multiple config are tested\n",
    "MAX_DISTANCE_OBJ_1=30.0\n",
    "MAX_DISTANCE_INTERSECT_1=20.0\n",
    "MAX_TIME_INTERSECT_OBJ_1=10.0\n",
    "\n",
    "#how many images in bird view you want to draw\n",
    "NUMBER_IMAGE=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3f54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#required if you are playing with libraries and changing them. This reloads libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f36ee733",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import tqdm\n",
    "import pandas\n",
    "import math\n",
    "import json\n",
    "from typing import Callable\n",
    "from nuscenes import NuScenes\n",
    "from nuscenes.eval.prediction.splits import *\n",
    "import nuscenes.eval.detection.config as cnfig\n",
    "from nuscenes.eval.detection.configs import *\n",
    "from nuscenes.eval.detection.data_classes import DetectionBox \n",
    "from nuscenes.eval.detection import *\n",
    "import nuscenes.eval.detection.algo as ag\n",
    "from nuscenes.eval.detection.data_classes import DetectionMetricData, DetectionConfig, DetectionMetrics, DetectionBox, \\\n",
    "    DetectionMetricDataList\n",
    "from nuscenes.eval.common.data_classes import EvalBoxes\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Callable, Tuple\n",
    "from nuscenes.eval.common.utils import center_distance, scale_iou, yaw_diff, velocity_l2, attr_acc, cummean\n",
    "import nuscenes.eval.detection.evaluate as dcl    \n",
    "from nuscenes.prediction import *\n",
    "from nuscenes.map_expansion.map_api import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a3dcbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 46.701 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 12.3 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "nuscenes = NuScenes('v1.0-trainval', dataroot=DATAROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d043295",
   "metadata": {},
   "outputs": [],
   "source": [
    "confvalue=cnfig.config_factory(\"detection_cvpr_2019\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1685adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of scene that compose the val set\n",
    "eval=val = \\\n",
    "    ['scene-0003', 'scene-0012', 'scene-0013', 'scene-0014', 'scene-0015', 'scene-0016', 'scene-0017', 'scene-0018',\n",
    "     'scene-0035', 'scene-0036', 'scene-0038', 'scene-0039', 'scene-0092', 'scene-0093', 'scene-0094', 'scene-0095',\n",
    "     'scene-0096', 'scene-0097', 'scene-0098', 'scene-0099', 'scene-0100', 'scene-0101', 'scene-0102', 'scene-0103',\n",
    "     'scene-0104', 'scene-0105', 'scene-0106', 'scene-0107', 'scene-0108', 'scene-0109', 'scene-0110', 'scene-0221',\n",
    "     'scene-0268', 'scene-0269', 'scene-0270', 'scene-0271', 'scene-0272', 'scene-0273', 'scene-0274', 'scene-0275',\n",
    "     'scene-0276', 'scene-0277', 'scene-0278', 'scene-0329', 'scene-0330', 'scene-0331', 'scene-0332', 'scene-0344',\n",
    "     'scene-0345', 'scene-0346', 'scene-0519', 'scene-0520', 'scene-0521', 'scene-0522', 'scene-0523', 'scene-0524',\n",
    "     'scene-0552', 'scene-0553', 'scene-0554', 'scene-0555', 'scene-0556', 'scene-0557', 'scene-0558', 'scene-0559',\n",
    "     'scene-0560', 'scene-0561', 'scene-0562', 'scene-0563', 'scene-0564', 'scene-0565', 'scene-0625', 'scene-0626',\n",
    "     'scene-0627', 'scene-0629', 'scene-0630', 'scene-0632', 'scene-0633', 'scene-0634', 'scene-0635', 'scene-0636',\n",
    "     'scene-0637', 'scene-0638', 'scene-0770', 'scene-0771', 'scene-0775', 'scene-0777', 'scene-0778', 'scene-0780',\n",
    "     'scene-0781', 'scene-0782', 'scene-0783', 'scene-0784', 'scene-0794', 'scene-0795', 'scene-0796', 'scene-0797',\n",
    "     'scene-0798', 'scene-0799', 'scene-0800', 'scene-0802', 'scene-0904', 'scene-0905', 'scene-0906', 'scene-0907',\n",
    "     'scene-0908', 'scene-0909', 'scene-0910', 'scene-0911', 'scene-0912', 'scene-0913', 'scene-0914', 'scene-0915',\n",
    "     'scene-0916', 'scene-0917', 'scene-0919', 'scene-0920', 'scene-0921', 'scene-0922', 'scene-0923', 'scene-0924',\n",
    "     'scene-0925', 'scene-0926', 'scene-0927', 'scene-0928', 'scene-0929', 'scene-0930', 'scene-0931', 'scene-0962',\n",
    "     'scene-0963', 'scene-0966', 'scene-0967', 'scene-0968', 'scene-0969', 'scene-0971', 'scene-0972', 'scene-1059',\n",
    "     'scene-1060', 'scene-1061', 'scene-1062', 'scene-1063', 'scene-1064', 'scene-1065', 'scene-1066', 'scene-1067',\n",
    "     'scene-1068', 'scene-1069', 'scene-1070', 'scene-1071', 'scene-1072', 'scene-1073']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ea963f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "476fc090",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "dt=dcl.DetectionEval(nuscenes,confvalue, RESULT_PATH, 'val', OUTPUT, verbose=False, MAX_DISTANCE_OBJ=MAX_DISTANCE_OBJ_1,\n",
    "                     MAX_DISTANCE_INTERSECT=MAX_DISTANCE_INTERSECT_1, MAX_TIME_INTERSECT_OBJ=MAX_TIME_INTERSECT_OBJ_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4672fb83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"sample_tokens = list(dt.sample_tokens)\\nsamples_json = {'sample_tokens': sample_tokens}\\n\\nsamples_root ='/cluster/work/andronn/MasterThesis/MASTER/master_repo/Thesis-Evaluating-Safety-Oriented-Metrics-for-Object-Detectors/results/'\\nwith open(os.path.join(samples_root, 'sample_tokens.json'), 'w') as f:\\n    json.dump(samples_json, f, indent=2)\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"sample_tokens = list(dt.sample_tokens)\n",
    "samples_json = {'sample_tokens': sample_tokens}\n",
    "\n",
    "samples_root ='/cluster/work/andronn/MasterThesis/MASTER/master_repo/Thesis-Evaluating-Safety-Oriented-Metrics-for-Object-Detectors/results/'\n",
    "with open(os.path.join(samples_root, 'sample_tokens.json'), 'w') as f:\n",
    "    json.dump(samples_json, f, indent=2)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b666e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing all BBs within 20m from ego at sample 28cce0b8fdb7437db096beb047179453\n",
      "Removing all BBs within 20m from ego at sample 645e0c2f9b724c5b95fe434930c2682a\n",
      "Adding FP at position (3, 5) relative to ego..\n",
      "Adding FP at position (0, 10) relative to ego..\n",
      "Adding FP at position (0, -20) relative to ego..\n",
      "Adding FP at position (5, 0) relative to ego..\n",
      "Removing all BBs within 20m from ego at sample c35efe5eaf784275b8c7e31fb50aa902\n",
      "Removing all BBs within 10m from ego at sample c752fd12565d48d598a8bfce5b0530ad\n",
      "Adding FP at position (0, 15) relative to ego..\n",
      "Removing all BBs within 37m from ego at sample d575e4dbdd6749068d4e71f7d99b02bc\n",
      "Adding FP at position (-3, 13) relative to ego..\n",
      "Removing first found BB within 20m from ego at sample f4a78e352cb74deca0bf4e1669aec42a\n"
     ]
    }
   ],
   "source": [
    "modified_predictions = True\n",
    "\n",
    "# Add False Positives and False Negatives to detection results\n",
    "if modified_predictions:\n",
    "    dt.add_FN('28cce0b8fdb7437db096beb047179453', 20) # Remove truck in front of vehicle in sample\n",
    "    dt.add_FN('645e0c2f9b724c5b95fe434930c2682a', 20) # Remove bus in front of vehicle in sample\n",
    "    dt.add_FP('6c8d4379e83646d08436f6ec92b35fe5', (3,5), (1.7, 4.0, 2.0)) # Add FP in front of ego\n",
    "    dt.add_FP('970e605b05c14027b08a6e74d5e8ceca', (0,10), (1.7, 4.0, 2.0)) # FP 10m in front of ego\n",
    "    dt.add_FP('b8849fcd11d14e499aea2ee258d8b581', (0,-20), (1.7, 4.0, 2.0)) # FP 20m behind ego\n",
    "    dt.add_FP('b8849fcd11d14e499aea2ee258d8b581', (5,0), (1.7, 4.0, 2.0))\n",
    "    dt.add_FN('c35efe5eaf784275b8c7e31fb50aa902', 20) # FNs for pedestrian detections\n",
    "    dt.add_FN('c752fd12565d48d598a8bfce5b0530ad', 10) # FNs for closest vehicles\n",
    "    dt.add_FP('d056b9bdd56f44669540c6c323042d30', (0,15), (1.7, 4.0, 2.0)) # FP 15m ahead of ego\n",
    "    dt.add_FN('d575e4dbdd6749068d4e71f7d99b02bc', 37) # FN for single vehicle detection\n",
    "    dt.add_FP('d056b9bdd56f44669540c6c323042d30', (-3,13), (1.7, 4.0, 2.0)) # FP in opposite lane\n",
    "    dt.add_FN('f4a78e352cb74deca0bf4e1669aec42a', 20, False) # Remove one! of the detections in front of ego \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27aac5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING EVALUATION in main (self)\n",
      "Made sample directory: /cluster/work/andronn/MasterThesis/MASTER/master_repo/Thesis-Evaluating-Safety-Oriented-Metrics-for-Object-Detectors/results/pointpillars-fpn/METRIC_SAMPLES_MODIFIED/ec78e68b1637464da5305a9cbed214c4\n",
      "\n",
      "Rendering sample token ec78e68b1637464da5305a9cbed214c4\n",
      "Rendering sample token ec78e68b1637464da5305a9cbed214c4\n",
      "Rendering sample token ec78e68b1637464da5305a9cbed214c4\n",
      "Rendering sample token ec78e68b1637464da5305a9cbed214c4\n",
      "Rendering sample token ec78e68b1637464da5305a9cbed214c4\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: truck\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: bus\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: trailer\n",
      "ALGO.PY WARNING : NO CRIT GT PREDICTIONS, CLASS: construction_vehicle\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: motorcycle\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: traffic_cone\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: barrier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/andronn/.local/lib/python3.8/site-packages/nuscenes/eval/detection/algo.py:277: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  prec_s = tp_s / (fp_s + tp_s)\n",
      "/cluster/home/andronn/.local/lib/python3.8/site-packages/nuscenes/eval/detection/algo.py:288: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  prec_s_crit= tp_gts_crit/(fp_s_crit + tp_s_crit)\n",
      "/cluster/home/andronn/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/cluster/home/andronn/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using model weights ./planner.pt\n",
      "using location masks ./masks_trainval.json\n",
      "calculating pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting 0 timestamps...\n",
      "Saved metric data for sample ec78e68b1637464da5305a9cbed214c4\n",
      "Sample ec78e68b1637464da5305a9cbed214c4 data saved.\n",
      "\n",
      "Made sample directory: /cluster/work/andronn/MasterThesis/MASTER/master_repo/Thesis-Evaluating-Safety-Oriented-Metrics-for-Object-Detectors/results/pointpillars-fpn/METRIC_SAMPLES_MODIFIED/28cce0b8fdb7437db096beb047179453\n",
      "\n",
      "Rendering sample token 28cce0b8fdb7437db096beb047179453\n",
      "Rendering sample token 28cce0b8fdb7437db096beb047179453\n",
      "Rendering sample token 28cce0b8fdb7437db096beb047179453\n",
      "Rendering sample token 28cce0b8fdb7437db096beb047179453\n",
      "Rendering sample token 28cce0b8fdb7437db096beb047179453\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: truck\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: trailer\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: construction_vehicle\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: motorcycle\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: bicycle\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: traffic_cone\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: barrier\n",
      "using model weights ./planner.pt\n",
      "using location masks ./masks_trainval.json\n",
      "calculating pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting 0 timestamps...\n",
      "Saved metric data for sample 28cce0b8fdb7437db096beb047179453\n",
      "Sample 28cce0b8fdb7437db096beb047179453 data saved.\n",
      "\n",
      "Made sample directory: /cluster/work/andronn/MasterThesis/MASTER/master_repo/Thesis-Evaluating-Safety-Oriented-Metrics-for-Object-Detectors/results/pointpillars-fpn/METRIC_SAMPLES_MODIFIED/6c8d4379e83646d08436f6ec92b35fe5\n",
      "\n",
      "Rendering sample token 6c8d4379e83646d08436f6ec92b35fe5\n",
      "Rendering sample token 6c8d4379e83646d08436f6ec92b35fe5\n",
      "Rendering sample token 6c8d4379e83646d08436f6ec92b35fe5\n",
      "Rendering sample token 6c8d4379e83646d08436f6ec92b35fe5\n",
      "Rendering sample token 6c8d4379e83646d08436f6ec92b35fe5\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: bus\n",
      "ALGO.PY WARNING : NO CRIT GT PREDICTIONS, CLASS: trailer\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: pedestrian\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: motorcycle\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: bicycle\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: traffic_cone\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: barrier\n",
      "using model weights ./planner.pt\n",
      "using location masks ./masks_trainval.json\n",
      "calculating pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting 0 timestamps...\n",
      "Saved metric data for sample 6c8d4379e83646d08436f6ec92b35fe5\n",
      "Sample 6c8d4379e83646d08436f6ec92b35fe5 data saved.\n",
      "\n",
      "Made sample directory: /cluster/work/andronn/MasterThesis/MASTER/master_repo/Thesis-Evaluating-Safety-Oriented-Metrics-for-Object-Detectors/results/pointpillars-fpn/METRIC_SAMPLES_MODIFIED/f4a78e352cb74deca0bf4e1669aec42a\n",
      "\n",
      "Rendering sample token f4a78e352cb74deca0bf4e1669aec42a\n",
      "Rendering sample token f4a78e352cb74deca0bf4e1669aec42a\n",
      "Rendering sample token f4a78e352cb74deca0bf4e1669aec42a\n",
      "Rendering sample token f4a78e352cb74deca0bf4e1669aec42a\n",
      "Rendering sample token f4a78e352cb74deca0bf4e1669aec42a\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: bus\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: trailer\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: construction_vehicle\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: pedestrian\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: traffic_cone\n",
      "ALGO.PY WARNING : NO GT PREDICTIONS, CLASS: barrier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/home/andronn/.local/lib/python3.8/site-packages/nuscenes/eval/detection/algo.py:230: RuntimeWarning: invalid value encountered in divide\n",
      "  prec_crit= tp_gt_crit/(fp_pred_crit + tp_pred_crit) #TODO: ha dato 1 warning su runtime value not valid???\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using model weights ./planner.pt\n",
      "using location masks ./masks_trainval.json\n",
      "calculating pkl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plotting 0 timestamps...\n",
      "Saved metric data for sample f4a78e352cb74deca0bf4e1669aec42a\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "\n",
    "ex_tokens_path='/cluster/work/andronn/MasterThesis/MASTER/master_repo/Thesis-Evaluating-Safety-Oriented-Metrics-for-Object-Detectors/results/sample_tokens_selected.json'\n",
    "\n",
    "\n",
    "dt.main(plot_examples=NUMBER_IMAGE,\n",
    "        render_curves=True, \n",
    "        model_name=DETECTOR,\n",
    "        MAX_DISTANCE_OBJ=MAX_DISTANCE_OBJ_1,\n",
    "        MAX_DISTANCE_INTERSECT=MAX_DISTANCE_INTERSECT_1,\n",
    "        MAX_TIME_INTERSECT=MAX_TIME_INTERSECT_OBJ_1,\n",
    "        recall_type=\"PRED AL NUMERATORE\",\n",
    "        save_metrics_samples=True,\n",
    "        samples_tokens_path=ex_tokens_path,\n",
    "        modified_predictions=modified_predictions) #This must be \"PRED AL NUMERATORE\", to match results of the paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb35f276",
   "metadata": {},
   "source": [
    "### Calculate PKLs\n",
    "Using planning_centric_metrics, calculate PKL for dataset (deprecated, PKL is calculated in calc_sample_crit for each sample for shorter computation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d7188c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dt.filter_boxes_confidence(conf_th=0.15) # Filter BBs on confidence before PKL evaluation. default = 0.15\\n\\nfrom nuscenes.map_expansion.map_api import NuScenesMap\\nfrom planning_centric_metrics import calculate_pkl\\nimport torch\\n\\nprint(torch.cuda.device_count())\\nprint(torch.cuda.is_available())\\ngpuid = -1\\ndevice = torch.device(f\\'cuda:{gpuid}\\') if gpuid >= 0        else torch.device(\\'cpu\\')\\nprint(\\'using device: {0}\\'.format(device))\\n\\nmap_folder = \\'/cluster/work/andronn/MasterThesis/MASTER/mmdetection3d/data/nuscenes/maps/\\'\\nnusc_maps = {map_name: NuScenesMap(dataroot=map_folder,\\n                 map_name=map_name) for map_name in [\\n                    \"singapore-hollandvillage\",\\n                    \"singapore-queenstown\",\\n                    \"boston-seaport\",\\n                    \"singapore-onenorth\",\\n                ]}\\nnworkers = 2\\n\\npkl = calculate_pkl(dt.gt_boxes, dt.pred_boxes,\\n                         dt.sample_tokens, dt.nusc,\\n                         nusc_maps, device,\\n                         nworkers=nworkers, bsz=16,\\n                         plot_kextremes=0,\\n                         verbose=True)\\nwith open(os.path.join(OUTPUT,\\'pkl_results.json\\'), \\'w\\') as fp:\\n    json.dump(pkl, fp)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"dt.filter_boxes_confidence(conf_th=0.15) # Filter BBs on confidence before PKL evaluation. default = 0.15\n",
    "\n",
    "from nuscenes.map_expansion.map_api import NuScenesMap\n",
    "from planning_centric_metrics import calculate_pkl\n",
    "import torch\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())\n",
    "gpuid = -1\n",
    "device = torch.device(f'cuda:{gpuid}') if gpuid >= 0\\\n",
    "        else torch.device('cpu')\n",
    "print('using device: {0}'.format(device))\n",
    "\n",
    "map_folder = '/cluster/work/andronn/MasterThesis/MASTER/mmdetection3d/data/nuscenes/maps/'\n",
    "nusc_maps = {map_name: NuScenesMap(dataroot=map_folder,\n",
    "                 map_name=map_name) for map_name in [\n",
    "                    \"singapore-hollandvillage\",\n",
    "                    \"singapore-queenstown\",\n",
    "                    \"boston-seaport\",\n",
    "                    \"singapore-onenorth\",\n",
    "                ]}\n",
    "nworkers = 2\n",
    "\n",
    "pkl = calculate_pkl(dt.gt_boxes, dt.pred_boxes,\n",
    "                         dt.sample_tokens, dt.nusc,\n",
    "                         nusc_maps, device,\n",
    "                         nworkers=nworkers, bsz=16,\n",
    "                         plot_kextremes=0,\n",
    "                         verbose=True)\n",
    "with open(os.path.join(OUTPUT,'pkl_results.json'), 'w') as fp:\n",
    "    json.dump(pkl, fp)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69690d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc2480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a8c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
